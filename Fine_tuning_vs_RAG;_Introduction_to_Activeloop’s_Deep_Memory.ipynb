{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONRxYIGDHNr7"
      },
      "source": [
        "# Deep Memory trained on Syntethic Queries improves recall@10 by +20%\n",
        "\n",
        "You need to have labelled data (query and relevance pairs) for training deep memory. However it is sometimes hard to obtain labelled data when you start fresh.\n",
        "\n",
        "In this tutorial we will take an existing dataset and generate queries using GPT to train Deep Memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayEgfW2vHPDK"
      },
      "source": [
        "## 0. Setup packages and credentials\n",
        "Install Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCZJ5PXlGMu6"
      },
      "outputs": [],
      "source": [
        "!pip3 install deeplake langchain openai tiktoken llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRYFBOBMHG16"
      },
      "source": [
        "Setup Activeloop and OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re7qIcjoGrdm"
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "os.environ['ACTIVELOOP_TOKEN'] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl6ceqfwGyS0"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVACLdPZHqwW"
      },
      "source": [
        "## 1. Load the dataset and create a Deep Lake vector store\n",
        "\n",
        "We are going to use GPT3.5 to generate questions based on the context provided by a chunk test."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!curl 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham/paul_graham_essay.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NCMGoopuG0t",
        "outputId": "219fce65-3be3-4102-f406-5edf8ee24ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 75042  100 75042    0     0   149k      0 --:--:-- --:--:-- --:--:--  149k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
        "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# By default, the node/chunks ids are set to random uuids. To ensure same id's per run, we manually set them.\n",
        "for idx, node in enumerate(nodes):\n",
        "    node.id_ = f\"node_{idx}\"\n",
        "\n",
        "print(f\"Number of Documents: {len(documents)}\")\n",
        "print(f\"Number of nodes: {len(nodes)} with the current chunk size of {node_parser.chunk_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2Q8HcqxuJiw",
        "outputId": "f2b9c51b-0e4a-4eb9-daa7-b35dcc87dc46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Documents: 1\n",
            "Number of nodes: 58 with the current chunk size of 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNbSMwL8OY_H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "0a913b6a9cf54491928938b4063f3627",
            "ca2a122fc07140409d43cef5b57949b0",
            "64fcc92639314a1eb49b960c12e5b32c",
            "5b10e8aa9d444c2b858e9f503e7a8a6e",
            "ebae56a36f264cdebd360c361d4b44c5",
            "cbc0baded0f148ea80968a5d8e97dc86",
            "1fb337657cbc42928d1f4c6aa68cd01c",
            "fdf8a02f88d74c99b45a5705caa155ec",
            "43134e0b0f864a6cbbc73d9ace20c754",
            "b5770d0e58c04663b95b304f3f8411ba",
            "e22fee56ebf64843a5f8206bc0d6c947"
          ]
        },
        "outputId": "b975d9f0-8557-4a67-e910-58eeffaf90f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r\r\r\r"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/58 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a913b6a9cf54491928938b4063f3627"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading data to deeplake dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:00<00:00, 274.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='./data/paul_graham/deep_lake_db', tensors=['text', 'metadata', 'embedding', 'id'])\n",
            "\n",
            "  tensor      htype      shape      dtype  compression\n",
            "  -------    -------    -------    -------  ------- \n",
            "   text       text      (58, 1)      str     None   \n",
            " metadata     json      (58, 1)      str     None   \n",
            " embedding  embedding  (58, 1536)  float32   None   \n",
            "    id        text      (58, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\r\r"
          ]
        }
      ],
      "source": [
        "from llama_index import VectorStoreIndex, ServiceContext, StorageContext\n",
        "from llama_index.vector_stores import DeepLakeVectorStore\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "# Create a DeepLakeVectorStore locally to store the vectors\n",
        "dataset_path = \"./data/paul_graham/deep_lake_db\"\n",
        "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True, exec_option=\"compute_engine\")\n",
        "\n",
        "# LLM that will answer questions with the retrieved context\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
        "embed_model = OpenAIEmbedding()\n",
        "\n",
        "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "vector_index = VectorStoreIndex(nodes, service_context=service_context, storage_context=storage_context, show_progress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's upload the local Vectore Store to Active Loop's platform and then convert it into a managed database."
      ],
      "metadata": {
        "id": "HKiyo5SpuxYD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yW2imseOuLG"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "local = \"./data/paul_graham/deep_lake_db\"\n",
        "hub_path = \"hub://genai360/LlamaIndex_paulgraham_essay\"\n",
        "hub_managed_path = \"hub://genai360/LlamaIndex_paulgraham_essay_managed\"\n",
        "\n",
        "# First upload our local vector store\n",
        "deeplake.deepcopy(local, hub_path, overwrite=True)\n",
        "# Create a managed vector store under a different name\n",
        "deeplake.deepcopy(hub_path, hub_managed_path, overwrite=True, runtime={\"tensor_db\": True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A-WROoEOz38"
      },
      "source": [
        "## 2. Generate a dataset of Queries and Documents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch dataset docs and ids if they exist (optional you can also ingest)\n",
        "db = DeepLakeVectorStore(dataset_path=hub_managed_path, overwrite=False, exec_option=\"compute_engine\", read_only=True,)\n",
        "docs = db.vectorstore.dataset.text.data(fetch_chunks=True, aslist=True)['value']\n",
        "ids = db.vectorstore.dataset.id.data(fetch_chunks=True, aslist=True)['value']\n",
        "print(len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_1d6IRqvvUU",
        "outputId": "c65d19b7-a711-4c3c-b8fd-5eb0b71cbf33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Lake Dataset in hub://genai360/LlamaIndex_paulgraham_essay_managed already exists, loading from the storage\n",
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7pjHtslc9Ao"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "def generate_question(text):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-1106\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a world class expert for generating questions based on provided context. \\\n",
        "                        You make sure the question can be answered by the text.\"},\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": text,\n",
        "                },\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except:\n",
        "        question_string = \"No question generated\"\n",
        "        return question_string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coaUWAp4ISyw"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_queries(docs: list[str], ids: list[str], n: int):\n",
        "\n",
        "    questions = []\n",
        "    relevances = []\n",
        "    pbar = tqdm(total=n)\n",
        "    while len(questions) < n:\n",
        "        # 1. randomly draw a piece of text and relevance id\n",
        "        r = random.randint(0, len(docs)-1)\n",
        "        text, label = docs[r], ids[r]\n",
        "\n",
        "        # 2. generate queries and assign and relevance id\n",
        "        generated_qs = [generate_question(text)]\n",
        "        if generated_qs == [\"No question generated\"]:\n",
        "            print(\"No question generated\")\n",
        "            continue\n",
        "\n",
        "        questions.extend(generated_qs)\n",
        "        relevances.extend([[(label, 1)] for _ in generated_qs])\n",
        "        pbar.update(len(generated_qs))\n",
        "\n",
        "    return questions[:n], relevances[:n]\n",
        "\n",
        "# Here we choose to generate 40 questions\n",
        "questions, relevances = generate_queries(docs, ids, n=40)\n",
        "print(len(questions))\n",
        "print(questions[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKETptufMlDb"
      },
      "source": [
        "## 3. Train Deep Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qhx2PjztMmdf"
      },
      "outputs": [],
      "source": [
        "job_id = db.vectorstore.deep_memory.train(\n",
        "    queries=questions,\n",
        "    relevance=relevances,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XneeHJdOE0g"
      },
      "outputs": [],
      "source": [
        "db.vectorstore.deep_memory.status('657c6edee8fc60c8a382221a')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFoi25nXXj4L"
      },
      "source": [
        "Wait until training status becomes completed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znNn87saMnr1"
      },
      "source": [
        "## 4. Evaluate Deep Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37vIbzAQSGD9"
      },
      "source": [
        "### 4.1 Manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgi-zdwuSBNj"
      },
      "outputs": [],
      "source": [
        "query = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53ht4F2fR97p"
      },
      "outputs": [],
      "source": [
        "db.similarity_search(query=query, deep_memory=False, k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxlN0VrER-ap"
      },
      "outputs": [],
      "source": [
        "db.similarity_search(query=query, deep_memory=True, k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZBoA8YqSI4b"
      },
      "source": [
        "### 4.2 Quantitative Evaluation on Synthetically generated queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd65U190Mp9v"
      },
      "outputs": [],
      "source": [
        "validation_questions, validation_relevances = generate_queries(docs, ids, n=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXJYCiwNPbOL"
      },
      "outputs": [],
      "source": [
        "recalls = db.vectorstore.deep_memory.evaluate(\n",
        "    queries=validation_questions,\n",
        "    relevance=validation_relevances,\n",
        "    embedding_function=openai_embeddings.embed_documents,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a913b6a9cf54491928938b4063f3627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca2a122fc07140409d43cef5b57949b0",
              "IPY_MODEL_64fcc92639314a1eb49b960c12e5b32c",
              "IPY_MODEL_5b10e8aa9d444c2b858e9f503e7a8a6e"
            ],
            "layout": "IPY_MODEL_ebae56a36f264cdebd360c361d4b44c5"
          }
        },
        "ca2a122fc07140409d43cef5b57949b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbc0baded0f148ea80968a5d8e97dc86",
            "placeholder": "​",
            "style": "IPY_MODEL_1fb337657cbc42928d1f4c6aa68cd01c",
            "value": "Generating embeddings: 100%"
          }
        },
        "64fcc92639314a1eb49b960c12e5b32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdf8a02f88d74c99b45a5705caa155ec",
            "max": 58,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43134e0b0f864a6cbbc73d9ace20c754",
            "value": 58
          }
        },
        "5b10e8aa9d444c2b858e9f503e7a8a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5770d0e58c04663b95b304f3f8411ba",
            "placeholder": "​",
            "style": "IPY_MODEL_e22fee56ebf64843a5f8206bc0d6c947",
            "value": " 58/58 [00:04&lt;00:00, 12.01it/s]"
          }
        },
        "ebae56a36f264cdebd360c361d4b44c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbc0baded0f148ea80968a5d8e97dc86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fb337657cbc42928d1f4c6aa68cd01c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdf8a02f88d74c99b45a5705caa155ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43134e0b0f864a6cbbc73d9ace20c754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5770d0e58c04663b95b304f3f8411ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e22fee56ebf64843a5f8206bc0d6c947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}